# Deploy OceanBase Database in a Kubernetes cluster

This topic describes how to use ob-operator to deploy OceanBase Database in a Kubernetes cluster. 

Like other operators, ob-operator aims to run OceanBase Database in a container of a Kubernetes cluster. ob-operator supports the creation and deletion of OceanBase clusters, complete lifecycle management for nodes, and OceanBase Database Proxy (ODP) management, and will support features such as tenant management and multiple Kubernetes clusters in the future. 

You can perform the following steps to use ob-operator to deploy OceanBase Database. 

## Deploy CRDs

```bash
kubectl apply -f https://raw.githubusercontent.com/oceanbase/ob-operator/master/deploy/crd.yaml
```

## Deploy ob-operator

### Deploy ob-operator based on the default configurations

```bash
kubectl apply -f https://raw.githubusercontent.com/oceanbase/ob-operator/master/deploy/operator.yaml
```

### Deploy ob-operator based on custom configurations

You can modify the `operator.yaml` file based on your configurations.
For example, you can add the startup parameter `--cluster-name` to the `operator.yaml` file. In this way, ob-operator processes only a custom resource definition (CRD) whose `cluster` value is the same as the value of the startup parameter `--cluster-name`. 

  <main id="notice" type='notice'>

    <h4>Notice</h4>

    <p>The value of <code>--cluster-name</code> must be the same as the value of <code>cluster</code> in the configurations of the OceanBase cluster. </p>

  </main>

Modify the `operator.yaml` file based on the following content:

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    control-plane: controller-manager
  name: ob-operator-controller-manager
  namespace: oceanbase-system
spec:
  replicas: 1
  selector:
    matchLabels:
      control-plane: controller-manager
  template:
    metadata:
      labels:
        control-plane: controller-manager
    spec:
      containers:
      - args:
        - --health-probe-bind-address=:8081
        - --metrics-bind-address=127.0.0.1:8080
        - --leader-elect
        - --cluster-name=cn
        command:
        - /manager
        image: ob-operator:latest
        imagePullPolicy: Always
        name: manager
```

## Configure labels for Kubernetes nodes

You must configure labels for Kubernetes nodes. The labels must match the value of `nodeSelector` in the `obcluster.yaml` file. ob-operator will schedule the Pods to nodes with the corresponding labels. 
We recommend that you specify the key of a label to `topology.kubernetes.io/zone`. We recommend that you configure different labels for different zones for disaster recovery. 

```bash
kubectl label node nodename topology.kubernetes.io/zone=zonename
kubectl label node ${node_name} topology.kubernetes.io/zone=${zone_name}
```

## Deploy local-path-provisioner

In the default configurations of ob-operator, local-path provisioner is used. Therefore, you must get it ready in advance.

Run the following command to deploy local-path-provisioner:

```text
kubectl apply -f https://raw.githubusercontent.com/rancher/local-path-provisioner/v0.0.22/deploy/local-path-storage.yaml
```

For more information, see <https://github.com/rancher/local-path-provisioner>.

## Deploy the OceanBase cluster

### Deploy the OceanBase cluster based on the default configurations

```bash
kubectl apply -f https://raw.githubusercontent.com/oceanbase/ob-operator/master/deploy/obcluster.yaml
```

### Deploy the OceanBase cluster based on custom configurations

You can modify the `obcluster.yaml` file based on your configurations. 
Modify the `operator.yaml` file based on the following content:

```yaml
apiVersion: cloud.oceanbase.com/v1
kind: OBCluster
metadata:
  name: ob-test
  namespace: obcluster
spec:
  imageRepo: oceanbasedev/oceanbase-cn
  tag: 4.0.0-snapshot-20230130110559
  clusterID: 1
  topology:
    - cluster: cn
      zone:
      - name: zone1
        region: region1
        nodeSelector:
          topology.kubernetes.io/zone: zone1
        replicas: 1
      - name: zone2
        region: region1
        nodeSelector:
          topology.kubernetes.io/zone: zone2
        replicas: 1
      - name: zone3
        region: region1
        nodeSelector:
          topology.kubernetes.io/zone: zone3
        replicas: 1
      parameters:
        - name: freeze_trigger_percentage
          value: "30"
  resources:
    cpu: 2
    memory: 10Gi
    storage:
      - name: data-file
        storageClassName: "local-path"
        size: 50Gi
      - name: data-log
        storageClassName: "local-path"
        size: 50Gi
      - name: log
        storageClassName: "local-path"
        size: 30Gi
```

Parameters:

- `imageRepo`: the image repository of OceanBase Database. 
- `tag`: the tag of the OceanBase image. 

- `cluster`: To deploy an OceanBase cluster in a Kubernetes cluster, set the value of this parameter to the same as that of the startup parameter `--cluster-name` of ob-operator. 
- `parameters`: the custom parameters of OceanBase Database. Specify the parameters as needed. 

- `cpu`: the number of CPU cores. We recommend that you set the value to an integer greater than 2. A value smaller than 2 may cause system exceptions. 

- `memory`: the memory size. We recommend that you set the value to an integer greater than 10 GiB. A value smaller than 10 GiB may cause system exceptions. 

- `data-file`: the data file size. The value must be the same as that of the `datafile_size` parameter of the OBServer node. We recommend that you set this parameter to a value that is three times the memory size. You can specify `storageClassName` as needed. 

- `data-log`: the data directory size. The value must be the same as the size of the `data_dir` directory of the OBServer node. We recommend that you set this parameter to a value that is five times the memory size. You can specify `storageClassName` as needed. 

- `log`: the size of the system log directory of the OBServer node. We recommend that you set a value greater than 30 GiB. You can specify `storageClassName` as needed. 

## Deploy ODP

### Deploy ODP based on the default configurations

```bash
kubectl apply -f https://raw.githubusercontent.com/oceanbase/ob-operator/master/deploy/obproxy/deployment.yaml
kubectl apply -f https://raw.githubusercontent.com/oceanbase/ob-operator/master/deploy/obproxy/service.yaml
```

### Deploy ODP based on custom configurations

You can modify the `obproxy.yaml` file based on your configurations. 
Modify the `operator.yaml` file based on the following content:

```yaml
# deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: obproxy
  namespace: obcluster
spec:
  selector:
    matchLabels:
      app: obproxy
  replicas: 3
  template:
    metadata:
      labels:
        app: obproxy
    spec:
      containers:
        - name: obproxy
          image: oceanbase/obproxy-ce:3.2.3
          ports:
            - containerPort: 2883
              name: "sql"
            - containerPort: 2884
              name: "prometheus"
          env:
            - name: APP_NAME
              value: helloworld
            - name: OB_CLUSTER
              value: ob-test
            - name: RS_LIST
              value: $(SVC_OB_TEST_SERVICE_HOST):$(SVC_OB_TEST_SERVICE_PORT)
          resources:
            limits:
              memory: 2Gi
              cpu: "1"
---
# service
apiVersion: v1
kind: Service
metadata:
  name: obproxy-service
  namespace: obcluster
spec:
  type: NodePort
  selector:
    app: obproxy
  ports:
    - name: "tcp"
      port: 2883
      targetPort: 2883
      nodePort: 30083
```

Environment variables:

- `APP_NAME`: the app name of ODP. 
- `OB_CLUSTER`: the name of the OceanBase cluster to which ODP connects. 

- `RS_LIST`: the Root Service list of the OceanBase cluster, which is generated based on the service address of the OceanBase cluster. After you deploy the OceanBase cluster, two environment variables are automatically generated: `SVC_OB_TEST_SERVICE_HOST` and `SVC_OB_TEST_SERVICE_PORT`. They respectively indicate the service address and port of the OceanBase cluster. 

## Deploy Prometheus

### Deploy Prometheus based on the default configurations

```bash
kubectl apply -f https://raw.githubusercontent.com/oceanbase/ob-operator/master/deploy/prometheus/cluster-role.yaml
kubectl apply -f https://raw.githubusercontent.com/oceanbase/ob-operator/master/deploy/prometheus/cluster-role-binding.yaml
kubectl apply -f https://raw.githubusercontent.com/oceanbase/ob-operator/master/deploy/prometheus/configmap.yaml
kubectl apply -f https://raw.githubusercontent.com/oceanbase/ob-operator/master/deploy/prometheus/deployment.yaml
kubectl apply -f https://raw.githubusercontent.com/oceanbase/ob-operator/master/deploy/prometheus/service.yaml
```

### Deploy Prometheus based on custom configurations

The request address used by Prometheus to query OBAgent is configured by using a ConfigMap, and the requests are filtered by service name and port name. If custom configurations have been specified for the OceanBase cluster or ODP cluster, you must specify a regular expression for the service name based on the actual situation. 

```yaml
# Key configurations in the ConfigMap
scrape_configs:
  - job_name: 'obagent-monitor-basic'
    kubernetes_sd_configs:
      - role: endpoints
    metrics_path: '/metrics/ob/basic'
    relabel_configs:
    - source_labels: [__meta_kubernetes_endpoints_name,__meta_kubernetes_endpoint_port_name]
      regex: svc-monitor-ob-test;monagent
      action: keep
  - job_name: 'obagent-monitor-extra'
    kubernetes_sd_configs:
      - role: endpoints
    metrics_path: '/metrics/ob/extra'
    relabel_configs:
    - source_labels: [__meta_kubernetes_service_name, __meta_kubernetes_pod_container_port_name]
      regex: svc-monitor-ob-test;monagent
      action: keep
  - job_name: 'obagent-monitor-host'
    kubernetes_sd_configs:
      - role: endpoints
    metrics_path: '/metrics/node/host'
    relabel_configs:
    - source_labels: [__meta_kubernetes_endpoints_name,__meta_kubernetes_endpoint_port_name]
      regex: svc-monitor-ob-test;monagent
      action: keep
  - job_name: 'proxy-monitor'
    kubernetes_sd_configs:
      - role: endpoints
    metrics_path: '/metrics'
    relabel_configs:
    - source_labels: [__meta_kubernetes_endpoints_name,__meta_kubernetes_endpoint_port_name]
      regex: obproxy-service;prometheus
      action: keep
```

You can specify the service name and port name based on the service discovery capability of Kubernetes. In this way, all endpoints can be automatically found. You can query the corresponding monitoring data based on the request path. 

Configuration description:

- Description of the service and port:

   - OceanBase Database: ob-operator will create a service based on the name of the deployed OceanBase cluster. The service name is in the format of `svc-monitor-${obcluster_name}`, and the port name is `monagent`. If you have specified a name for the OceanBase cluster during deployment, you must modify the corresponding configurations based on the actual cluster name.
   - ODP: You must configure the service name and port name for ODP.

- Request paths of monitoring metrics:

   - Monitoring metrics of OceanBase Database: `/metrics/ob/basic` and `/metrics/ob/extra`

   - Monitoring metrics of ODP: `/metrics`. ODP supports the exposure of monitoring metrics by using the Prometheus protocol.

### Perform verification

Enter the Prometheus address in your browser and press Enter. On the page that appears, choose Status > Targets and verify whether the endpoints under `proxy-monitor`, `obagent-monitor-host`, `obagent-monitor-basic`, and `obagent	monitor-extra` are all in the UP state.
Click Graph and enter a `PromQL` expression to query the configured parameters.

## Deploy Grafana

### Deploy Grafana based on the default configurations

```bash
kubectl apply -f https://raw.githubusercontent.com/oceanbase/ob-operator/master/deploy/grafana/configmap.yaml
kubectl apply -f https://raw.githubusercontent.com/oceanbase/ob-operator/master/deploy/grafana/pvc.yaml
kubectl apply -f https://raw.githubusercontent.com/oceanbase/ob-operator/master/deploy/grafana/deployment.yaml
kubectl apply -f https://raw.githubusercontent.com/oceanbase/ob-operator/master/deploy/grafana/service.yaml
```

### Deploy Grafana based on custom configurations

Key configuration information of Grafana, including the address of the Prometheus data source, is set by using a ConfigMap. If Prometheus is deployed based on custom configurations, you must specify the actual service address.

```yaml
# Key configurations in the ConfigMap
apiVersion: v1
kind: ConfigMap
metadata:
  name: grafana-datasources
  namespace: obcluster
data:
  prometheus.yaml: |-
    {
        "apiVersion": 1,
        "datasources": [
            {
               "access":"proxy",
                "editable": true,
                "name": "prometheus",
                "orgId": 1,
                "type": "prometheus",
                "url": "http://svc-prometheus.obcluster.svc:8080",
                "version": 1,
                "isDefault": true
            }
        ]
    }
```

Configuration description:

- `url`: The service address of Prometheus will be used as the URL of the data source in the configurations of Grafana. You must specify the actual service address of Prometheus.

### Perform verification

Enter the Grafana address in your browser and press **Enter**. On the page that appears, enter the username and default password `admin` to log on. When you log on for the first time, you will be prompted to change the password.
Three configured dashboards are available. You can directly import them.

- oceanbase: 15215
- Host: 15216
- obproxy: 15354
   After the import, you can view the corresponding monitoring charts.

  <main id="notice" type='explain'>

    <h4>Note</h4>

    <p>Monitoring data will be displayed only after ODP receives actual requests.</p>

  </main>
