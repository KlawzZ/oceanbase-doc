# 在 Kubernetes 集群中部署 OceanBase 数据库

本文介绍如何通过 ob-operator 在 Kubernetes 集群中部署 OceanBase 数据库。使用 ob-operator 可以简化 OceanBase 在 Kubernetes 中的部署和运维。

## 部署 ob-operator

ob-operator 可以通过以下两种方式进行部署。

### 使用 Helm 部署

您可执行以下命令部署 ob-operator。

```shell
helm repo add ob-operator https://oceanbase.github.io/ob-operator/
helm install ob-operator ob-operator/ob-operator --namespace=oceanbase-system --create-namespace  --version=1.1.0
```

### 使用配置文件部署

您可执行如下命令使用默认配置部署 ob-operator。

```shell
# deploy CRD
kubectl apply -f https://raw.githubusercontent.com/oceanbase/ob-operator/master/deploy/crd.yaml

# deploy ob-operator
kubectl apply -f https://raw.githubusercontent.com/oceanbase/ob-operator/master/deploy/operator.yaml
```

#### ob-operator 自定义

如果您需要对 ob-operator 进行自定义修改，可执行如下命令下载配置文件。

```shell
# download the config file
wget https://raw.githubusercontent.com/oceanbase/ob-operator/master/deploy/crd.yaml
wget https://raw.githubusercontent.com/oceanbase/ob-operator/master/deploy/operator.yaml
```

根据自身需求修改配置文件后，执行如下命令进行自定义部署。

```shell
# after making some modification
kubectl apply -f crd.yaml
kubectl apply -f operator.yaml
```

## 部署 OceanBase 集群

### 部署前准备

#### 配置 Kubernetes 节点 label

ob-operator 会根据配置文件中的 `nodeSelector` 来决定 observer 节点的分布，因此需要首先对 Kubernetes 的 node 节点配置 label, Kubernetes 中一般会有可用区的 label `topology.kubernetes.io/zone`，可以直接使用这个 label，或者单独为 OceanBase 设置 node 节点的 label。
<!-- 怎么直接使用这个 label，是  kubectl label node node1 topology.kubernetes.io/zone=zone1 这样么-->
您可以通过如下命令来设置 node 的 label。

```shell
kubectl label node ${node_name} ${label_key}=${label_value}

# for example
kubectl label node node1 ob.zone=zone1
```

#### 部署 local-path-provisioner

ob-operator 在部署 OceanBase 数据库时需要创建 PVC 作为 OceanBase 数据库的存储，出于性能考虑，推荐使用本地存储, 本文中使用 local-path-provisioner 来管理 PVC。

部署命令如下：
<!-- 需确认是否存在适配性问题，每一个版本都可以使用 24 版本的 local-path-provisioner 么-->
```text
原：kubectl apply -f https://raw.githubusercontent.com/rancher/local-path-provisioner/v0.0.22/deploy/local-path-storage.yaml
现：kubectl apply -f https://raw.githubusercontent.com/rancher/local-path-provisioner/v0.0.24/deploy/local-path-storage.yaml
```

更多信息可以参考 [local-path-provisioner](https://github.com/rancher/local-path-provisioner)。

### 部署 OceanBase 集群
<!-- 部署 OB集群和 ODP 时是否支持使用默认配置进行部署 -->
OceanBase 集群通过 yaml 配置文件进行定义，您可参考 ob-operator 提供的配置文件进行自定义的修改。您可通过如下命令下载 obcluster 的配置文件。

```bash
kubectl https://raw.githubusercontent.com/oceanbase/ob-operator/master/deploy/obcluster.yaml
```

配置文件内容示例如下。

```yaml
apiVersion: cloud.oceanbase.com/v1
kind: OBCluster
metadata:
  name: ob-test
  namespace: obcluster
spec:
  imageRepo: oceanbasedev/oceanbase-cn
  tag: v4.1.0.0-100000192023032010
  imageObagent: oceanbase/obagent:1.2.0
  clusterID: 1
  topology:
    - cluster: cn
      zone:
      - name: zone1
        region: region1
        nodeSelector:
          ob.zone: zone1
        replicas: 1
      - name: zone2
        region: region1
        nodeSelector:
          ob.zone: zone2
        replicas: 1
      - name: zone3
        region: region1
        nodeSelector:
          ob.zone: zone3
        replicas: 1
      parameters:
        - name: log_disk_size
          value: "40G"
  resources:
    cpu: 2
    memory: 10Gi
    storage:
      - name: data-file
        storageClassName: "local-path"
        size: 50Gi
      - name: data-log
        storageClassName: "local-path"
        size: 50Gi
      - name: log
        storageClassName: "local-path"
        size: 30Gi
      - name: obagent-conf-file
        storageClassName: "local-path"
        size: 1Gi
    volume:
        name: backup
        nfs:
          server: ${nfs_server_address}
          path: /opt/nfs
          readOnly: false
```

主要配置介绍如下。

- `imageRepo`：OceanBase 镜像 repo。

- `tag`：OceanBase 镜像 tag。

- `imageObagent`: OBAgent 的镜像，用来做 OceanBase 集群的监控数据采集。
  
- `cluster`：按需配置，如果需要在该 Kubernetes 中部署 OceanBase 集群，请将 cluster 配置为与 ob-operator 启动参数 `--cluster-name` 相同（默认值为 `cn`）。

- `nodeSelector`: 当前 Zone 的 node 选择策略，按照 node 的 label 进行匹配。

- `parameters`：按需配置, 自定义的 OceanBase 数据库配置项。
  
- `cpu`：Pod 的 CPU 资源，建议配置为大于 2 的整数，小于 2 会引发系统异常。
  
- `memory`：Pod 的 Memory 资源，建议配置值大于 10 Gi，小于 10 Gi 会引发系统异常。
  
- `data-file`：OceanBase 数据存储配置，可以指定大小和 `storageClass`，用作 OceanBase 的数据目录，建议至少配置为 `memory` 大小的 3 倍以上。
  
- `data-log`：OceanBase 日志存储配置, 可以指定大小和 `storageClass`，用作 OceanBase 的 Clog 目录，建议大小至少配置为 `memory` 大小的 5 倍以上。

- `obagent-conf-file`：OBAgent 配置文件目录存储配置, 用来存放 OBAgent 配置文件, 不需要配置特别大，一般配置大小为 1Gi 就可以。
  
- `log`：用作备份数据的存储，无备份需求可以不配置，但是集群创建好之后无法再另外添加，需要提前做好规划。

配置文件修改之后可以通过如下命令进行部署：

```shell
kubectl apply -f obcluster.yaml
```

### 部署 ODP
<!-- 是否支持使用默认配置部署 ODP，使用默认配置时，RS_LIST 如何配置 -->
ODP 通过 yaml 配置文件进行定义，您可参考 ob-operator 提供的配置文件进行自定义的修改，通过如下命令下载 ODP 的配置文件：

```shell
wget https://raw.githubusercontent.com/oceanbase/ob-operator/master/deploy/obproxy/deployment.yaml
wget https://raw.githubusercontent.com/oceanbase/ob-operator/master/deploy/obproxy/service.yaml
```

deployment 配置文件内容如下：

```yaml
# deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: obproxy
  namespace: obcluster
spec:
  selector:
    matchLabels:
      app: obproxy
  replicas: 2
  template:
    metadata:
      labels:
        app: obproxy
    spec:
      containers:
        - name: obproxy
          image: oceanbasedev/obproxy-ce:4.1.0.0-7
          ports:
            - containerPort: 2883
              name: "sql"
            - containerPort: 2884
              name: "prometheus"
          env:
            - name: APP_NAME
              value: helloworld
            - name: OB_CLUSTER
              value: ob-test
            - name: RS_LIST
              value: $(SVC_OB_TEST_SERVICE_HOST):$(SVC_OB_TEST_SERVICE_PORT)
          resources:
            limits:
              memory: 2Gi
              cpu: "1"
```

主要环境变量配置说明：

- APP_NAME：ODP 的应用名称。

- OB_CLUSTER：ODP 连接的 OceanBase 集群名。
<!-- 介绍没有看懂需要怎么配置 -->
- RS_LIST：OceanBase 集群的 rs_list，这里是通过两个环境变量 `SVC_OB_TEST_SERVICE_HOST` 和 `SVC_OB_TEST_SERVICE_PORT` 来配置，环境变量的名字和 OceanBase 数据库的集群名有关，其中 `OB_TEST` 部分是 OceanBase 集群名，需要转换成大写和下划线。
<!-- 原本的介绍，所以这两个环境变量会自动生成么，生成后配置文件会自动调用？ -->
- `RS_LIST`: OceanBase 集群的 rs_list, 这里是通过 OceanBase 集群的 service 地址来生成的，当部署好 OceanBase 集群之后，会自动产生两个环境变量 `SVC_OB_TEST_SERVICE_HOST`, `SVC_OB_TEST_SERVICE_PORT`, 分别表示 OceanBase 集群的服务地址和端口。

service 配置中会开放两个端口，一个用来作 sql 连接的端口，一个是监控数据的采集端口。service 配置文件内容如下：

```yaml
# service.yaml
apiVersion: v1
kind: Service
metadata:
  name: obproxy-service
  namespace: obcluster
spec:
  type: NodePort
  selector:
    app: obproxy
  ports:
    - name: "sql"
      port: 2883
      targetPort: 2883
      nodePort: 30083
    - name: "prometheus"
      port: 2884
      targetPort: 2884
      nodePort: 30084
```

### 连接 OceanBase 集群

推荐通过 ODP 来连接 OceanBase，在部署好 OceanBase 数据库和 ODP 后，您可通过以下命令来获取 ODP 的 service 连接地址。
<!-- 这里的 1.1.1.1 是示例 IP 对么 -->
```shell
kubectl get svc ${servicename} -n ${namespace}

# for example
kubectl get svc obproxy-service -n obcluster

# output
NAME              TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)                         AGE
obproxy-service   NodePort   1.1.1.1   <none>        2883:30083/TCP,2884:30084/TCP   1m
```

您可以通过 CLUSTER-IP 和 PORT 的方式进行连接，对应的连接命令如下。

```shell
# without password
mysql -h1.1.1.1 -P2883 -uroot oceanbase -A -c

# with password
mysql -h1.1.1.1 -P2883 -uroot -p oceanbase -A -c
```

## 监控 OceanBase 数据库

### 部署 prometheus
<!-- 是否支持使用默认配置进行部署 -->
```bash
kubectl apply -f https://raw.githubusercontent.com/oceanbase/ob-operator/master/deploy/prometheus/cluster-role.yaml
kubectl apply -f https://raw.githubusercontent.com/oceanbase/ob-operator/master/deploy/prometheus/cluster-role-binding.yaml
kubectl apply -f https://raw.githubusercontent.com/oceanbase/ob-operator/master/deploy/prometheus/configmap.yaml
kubectl apply -f https://raw.githubusercontent.com/oceanbase/ob-operator/master/deploy/prometheus/deployment.yaml
kubectl apply -f https://raw.githubusercontent.com/oceanbase/ob-operator/master/deploy/prometheus/service.yaml
```

### 自定义部署 prometheus

prometheus 查询 OBAgent 的请求地址通过 configmap 来进行配置，根据服务名，端口名来进行过滤, 如果 OceanBase 集群或者 ODP 集群进行过自定义的配置，需要根据实际情况设置服务名的正则表达式。

```yaml
# configmap 中关键配置
scrape_configs:
  - job_name: 'obagent-monitor-basic'
    kubernetes_sd_configs:
      - role: endpoints
    metrics_path: '/metrics/ob/basic'
    relabel_configs:
    - source_labels: [__meta_kubernetes_endpoints_name,__meta_kubernetes_endpoint_port_name]
      regex: svc-monitor-ob-test;monagent
      action: keep
  - job_name: 'obagent-monitor-extra'
    kubernetes_sd_configs:
      - role: endpoints
    metrics_path: '/metrics/ob/extra'
    relabel_configs:
    - source_labels: [__meta_kubernetes_service_name, __meta_kubernetes_pod_container_port_name]
      regex: svc-monitor-ob-test;monagent
      action: keep
  - job_name: 'obagent-monitor-host'
    kubernetes_sd_configs:
      - role: endpoints
    metrics_path: '/metrics/node/host'
    relabel_configs:
    - source_labels: [__meta_kubernetes_endpoints_name,__meta_kubernetes_endpoint_port_name]
      regex: svc-monitor-ob-test;monagent
      action: keep
  - job_name: 'proxy-monitor'
    kubernetes_sd_configs:
      - role: endpoints
    metrics_path: '/metrics'
    relabel_configs:
    - source_labels: [__meta_kubernetes_endpoints_name,__meta_kubernetes_endpoint_port_name]
      regex: obproxy-service;prometheus
      action: keep
```

利用 k8s 的服务发现能力，配置服务名称和端口名，可以自动找到所有的 endpoint 节点, 根据对应的请求路径，可以查询到对应的监控数据。

配置说明：

- service 和 port 说明:

  - oceanbase: ob-operator 会根据部署的 OceanBase 集群名称来创建一个 service， 命名的规则是 `svc-monitor-${obcluster_name}`, 端口名为 `monagent`, 如果部署 OceanBase 集群时自定义了集群名，需要根据实际的集群名称来修改对应配置
  
  - obproxy: 需要配置对应的 ODP 的 service 和 port name

- 监控指标的请求路径:

  - OceanBase 集群监控指标： 请求路径分别为 `/metrics/ob/basic`, `/metrics/ob/extra`
  
  - ODP 集群监控指标: ODP 自身支持了 prometheus 协议暴露监控指标的能力，请求路径为 `/metrics`
  
### 验证

浏览器打开 prometheus 的地址，查看 Status -> Targets, 验证 `proxy-monitor`, `obagent-monitor-host`, `obagent-monitor-basic`, `obagent-monitor-extra` 下的 Endpoint 都是 up 状态
点击 Graph, 可以输入 `PromQL` 表达式进行查询

### 部署 grafana
<!-- 是否支持默认配置部署 -->
#### 使用默认配置部署 grafana

```bash
kubectl apply -f https://raw.githubusercontent.com/oceanbase/ob-operator/master/deploy/grafana/configmap.yaml
kubectl apply -f https://raw.githubusercontent.com/oceanbase/ob-operator/master/deploy/grafana/pvc.yaml
kubectl apply -f https://raw.githubusercontent.com/oceanbase/ob-operator/master/deploy/grafana/deployment.yaml
kubectl apply -f https://raw.githubusercontent.com/oceanbase/ob-operator/master/deploy/grafana/service.yaml
```

### 自定义部署 grafana

grafana 通过 configmap 设置关键配置信息，包括 prometheus 数据源的地址, 如果 prometheus 进行了自定义部署，需要配置实际的 service 地址

```yaml
# configmap 中关键配置
apiVersion: v1
kind: ConfigMap
metadata:
  name: grafana-datasources
  namespace: obcluster
data:
  prometheus.yaml: |-
    {
        "apiVersion": 1,
        "datasources": [
            {
               "access":"proxy",
                "editable": true,
                "name": "prometheus",
                "orgId": 1,
                "type": "prometheus",
                "url": "http://svc-prometheus.obcluster.svc:8080",
                "version": 1,
                "isDefault": true
            }
        ]
    }
```

配置说明：

- `url`：Grafana 的配置中会使用到 prometheus 的服务地址作为数据源的配置(datasource 中的 url 字段)，需要根据 prometheus 的实际部署情况来填写

### 验证

浏览器打开 Grafana 的地址，通过 `admin` 用户登陆，默认密码也是 `admin`，第一次登录会提示修改密码。
有三个配置好的 dashboard 可以直接导入

- oceanbase: 15215

- 主机: 15216

- obproxy: 15354

导入之后，可以查看对应的监控图表

  <main id="notice" type='explain'>
    <h4>说明</h4>
    <p>ODP 需要有实际请求之后，才会有监控数据展示</p>
  </main>
